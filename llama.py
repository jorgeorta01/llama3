# -*- coding: utf-8 -*-
"""llama

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1soGF9uWnKvErvPLU5FU-u_gON3F4d18h

LLAMA
"""



import torch
import transformers
from transformers import AutoModelForCasualLM, AutoTokenizer
import os

HF_TOKEN = os.getenv("HF_TOKEN")
if HF_TOKEN:
    from huggingface_hub import
    login(token=HF_TOKEN)

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs=(
        {"torch_dtype": torch.float16}
        if torch.cuda.is_available()
        else {"device_map": "auto", "load_in_8bit": True}
    )
)

messages = [
    # {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Who won the world series in 2020?"},
]

prompt = pipeline.tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = pipeline(
    prompt,
    max_new_tokens=256,
    do_sample=True,
    top_p=0.9,
    temperature=0.6
)

print(outputs[0]["generated_text"][len(prompt):])